---
title: "Mlflow"
lang: en 
subtitle: "Deploy Models"
author: Jan Kirenz
execute:
  eval: false
  echo: true
highlight-style: github
format:
  revealjs: 
    toc: true
    toc-depth: 1
    embed-resources: false
    theme: [dark, custom.scss]  
    incremental: false
    transition: slide
    transition-speed: slow
    background-transition: fade
    code-copy: true
    code-line-numbers: true
    smaller: false
    scrollable: true
    slide-number: c
    preview-links: auto
    chalkboard: 
      buttons: false
   #logo: images/logo.png
   #footer: Setup | Jan Kirenz
---



# Python setup

```{python}

import numpy as np
from pydantic import BaseModel
from fastapi import FastAPI, HTTPException
import mlflow.pyfunc
import mlflow
from mlflow.tracking import MlflowClient


```


## Move model to production 

-After registering and staging the model, you can deploy it to various production environments 

- MLflow supports multiple deployment options, such as  
  - local deployment
  - deployment to cloud platforms like Microsoft Azure ML, Amazon SageMaker
  - using custom deployment solutions.


## Move model to production 

- First, import the required libraries (see Python Setup)

. . .

```{python}

# Create an MlflowClient instance
client = MlflowClient()

# Model information
REGISTERED_MODEL_NAME = "IrisClassifier"
MODEL_VERSION = 3

# Update the model version's stage to "Production"
client.transition_model_version_stage(
    name=REGISTERED_MODEL_NAME,
    version=MODEL_VERSION,
    stage="Production"
)

print(
    f"Model version {MODEL_VERSION} has been transitioned to Production stage")


```

# Deploy the model with FastAPI


## Create an API with FastAPI

- You'll need the conda environment [fastapi-env](https://github.com/kirenz/environments/blob/main/env-fastapi.yml)

- Next, you can create a FastAPI app to serve the MLflow model. 

- Take a look at a the file named `app.py` 

## Run the App

- Open a terminal and navigate to the directory containing your app.py file

- Run the following command:

. . .

```{bash}
uvicorn app:app --host 0.0.0.0 --port 8000
```

<!--
This command starts the Uvicorn server with your FastAPI app on port 8000. The --host 0.0.0.0 flag allows connections from any IP address, making the app accessible on your local network. You can change the port number if needed.
-->

## Access the API

- For API documentation, go to <http://localhost:8000/docs> or <http://127.0.0.1:8000/docs>

## Make a Prediction

- Make a POST request to the /predict endpoint
Example using curl:

```{bash}
!curl -X POST "http://localhost:8000/predict" -H "accept:

```

- This command sends a JSON object with the input data to the /predict endpoint and returns the prediction as a JSON response.

## Deploy with model monitoring using MySQL

- Follow the setup instructions to create a MySQL-Database

- Create a .env file in your project directory with the following content:

```{bash}
DB_USERNAME=your_username
DB_PASSWORD=your_password
DB_HOST=your_host
DB_NAME=your_dbname

```

## 

- Take a look at the file app_monitor

## Run the App

- Open a terminal and navigate to the directory containing the app_monitor.py file

- Run the following command:

. . .

```{bash}
uvicorn app_monitor:app --host 0.0.0.0 --port 8000
```

## Make a request 


```{bash}
curl -X POST "http://localhost:8000/predict" -H "Content-Type: application/json" -d '{"sepal_length": 5.1, "sepal_width": 3.5, "petal_length": 1.4, "petal_width": 0.2}'

```

- This command sends a POST request with the input features as a JSON payload in the request body.

# Deploy model with Streamlit

##

- Use the streamlit-env environment

- Take a look at the Python file named streamlit_app.py

# Further topics

## Monitor the model

- Track model performance, resource usage, and other relevant metrics
- Identify potential issues and maintain a high-quality service

## Implement a maintenance plan

- Regularly retrain the model with the latest data
- Address issues identified during monitoring
- Update model or infrastructure as needed

## Establish a model update process

- Automate the model training, evaluation, and deployment pipeline
- Transition new model versions through appropriate stages in MLflow

## Implement feedback loops

- Collect user feedback or additional data to improve the model
- Identify areas for performance enhancement or potential issues to address







<!--

## MLFlow UI

To mark a specific model version as "Production," you can also use the MLflow UI.

In the MLflow UI:

Locate the desired model version in the Model Version Management page.
Click on the three-dot menu on the right side of the row for the specific model version.
Select "Transition to -> Production" from the menu.

-->